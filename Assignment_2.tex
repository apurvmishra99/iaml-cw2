%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                IAML 2020 Assignment 2                %
%                                                      %
%                                                      %
% Authors: Hiroshi Shimodaira and JinHong Lu           %
% Based on: Assignment 1 by Oisin Mac Aodha, and       %
%          Octave Mariotti                             %
% Using template from: Michael P. J. Camilleri and     %
% Traiko Dinev.                                        %
%                                                      %
% Based on the Cleese Assignment Template for Students %
% from http://www.LaTeXTemplates.com.                  %
%                                                      %
% Original Author: Vel (vel@LaTeXTemplates.com)        %
%                                                      %
% License:                                             %
% CC BY-NC-SA 3.0                                      %
% (http://creativecommons.org/licenses/by-nc-sa/3.0/)  %
%                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%--------------------------------------------------------
%   IMPORTANT: Do not touch anything in this part
\documentclass[12pt]{article}
\input{style.tex}



% Options for Formatting Output

\global\setbool{clearon}{true} %
\global\setbool{authoron}{true} %
\ifbool{authoron}{\rhead{\small{\assignmentAuthorName}}\cfoot{\small{\assignmentAuthorName}}}{\rhead{}}



\newcommand{\assignmentQuestionName}{Question}
\newcommand{\assignmentTitle}{Assignment\ \#2}

\newcommand{\assignmentClass}{IAML -- INFR10069 (LEVEL 10)}

\newcommand{\assignmentWarning}{NO LATE SUBMISSIONS} % 
\newcommand{\assignmentDueDate}{Monday,\ November\ 23,\ 2020 @ 16:00}
%--------------------------------------------------------



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% NOTE: YOU NEED TO ENTER YOUR STUDENT ID BELOW.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% --------------------------------------------------------
% IMPORTANT: Specify your Student ID below. You will need to uncomment the line, else compilation will fail. Make sure to specify your student ID correctly, otherwise we may not be able to identify your work and you will be marked as missing.
\newcommand{\assignmentAuthorName}{s1864480}
%--------------------------------------------------------



\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%============================================================================%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
%
% Question 1
%

\begin{question}{(30 total points) Image data analysis with PCA}

  
  \questiontext{In this question we employ PCA to analyse image data}
  

  
  \medskip

   %==============================
   % Q1.1
  \begin{subquestion}{(3 points)
      Once you have applied the normalisation from Step 1 to Step 4 above,
      report the values of the first 4 elements for the first training
      sample in \texttt{Xtrn\_nm},
      i.e. \texttt{Xtrn\_nm[0,:]} and the last training sample,
      i.e. \texttt{Xtrn\_nm[-1,:]}.
    } \label{Q1.1}
    

      \begin{answerbox}{10em}
         The first 4 elements of the first and the last training sample are:
         $$[-0.    , -0.    , -0.0001, -0.0004]$$
         \newline
         \textit{Note: Values of the elements are rounded to 4 decimal places.}
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   % 
   % Q1.2
   \begin{subquestion}{(4 points)
      Using {\tt Xtrn} and Euclidean distance
      measure, for each class,
      find the two closest samples and two furthest
      samples of that class to the mean vector of the class.
    }  \label{Q1.2}




  \begin{answerbox}{52em}
    \begin{center}
        \includegraphics[width=0.75\textwidth, height=39em]{results/1_2.png}
    \end{center}
    The mean image of each data class shows us the shape and general outline/boundary of all the images in it. We can visually see the significantly large differences between the closest and furthest images to the mean. The closest images retain the shape and outline of their corresponding mean class which was expected as they are the nearest samples to it. In these samples the "dark" and "light" areas match the mean images "dark" and "light" areas. Whereas when we look at the furthest samples we can clearly see that they have a different boundary/outline, general shape, and big and sometimes significant differences in their "dark"/"light" areas. 
  \end{answerbox}



   \end{subquestion}

   % 
   % Q1.3
   \begin{subquestion}{(3 points)
       Apply Principal Component Analysis (PCA) to the data of {\tt
         Xtrn\_nm} using
       \href{https://scikit-learn.org/0.19/modules/generated/sklearn.decomposition.PCA.html}{sklearn.decomposition.PCA},
       and report the variances of projected data for the first five principal
       components in a table. Note that you should use {\tt Xtrn\_nm} instead of {\tt Xtrn}.} 
    \label{Q1.pca.variance}
     


    \begin{answerbox}{15em}
        \centering
        \begin{tabular}{|c|l|}
            \hline
            \multicolumn{1}{|l|}{\textbf{Principal Component (PC)}} & \textbf{Variance} \\ \hline
            PC1                                            & 19.8098  \\ \hline
            PC2                                            & 12.1122  \\ \hline
            PC3                                            & 4.1062   \\ \hline
            PC4                                            & 3.3818   \\ \hline
            PC5                                            & 2.624    \\ \hline
        \end{tabular}
    \end{answerbox}
    


   \end{subquestion}

   %==============================
   % Q1.4
   \begin{subquestion}{(3 points)
       Plot a graph of the cumulative explained variance ratio as a
       function of the number of principal components, $K$, where $1
       \le K \le 784$.
       Discuss the result briefly.
       } \label{Q1.plot.pca.variance}
   

      \begin{answerbox}{30em}
        \begin{center}
            \includegraphics[width=0.72\textwidth]{results/1_4.png}
        \end{center}
        This curve shows how much of the total, 784-dimensional variance is contained within the first $N$ components. The curve is extremely steep in the beginning and quickly plateaus. Here we can see that approximately first $20$ components contain $80\%$ of the variance, the first $100$ components contain approximately $90\%$ of the variance, and we would need around $500$ components to describe close to $100\%$ of the variance. This knowledge allows us to work with and explore the dataset more easily, as we can project our $784$ to much smaller dimensions, for eg. $100$ without losing much information. 
      \end{answerbox}
  


   \end{subquestion}

   %==============================
   % Q1.5
   \begin{subquestion}{(4 points)
      Display the images of the first 10 principal components in
      a 2-by-5 grid, putting the image of 1st principal component on
      the top left corner, followed by the one of 2nd component to the right.
      Discuss your findings briefly.
     } \label{Q1.disp.pca}
   

      \begin{answerbox}{35em}
         \begin{center}
            \includegraphics[width=0.9\textwidth]{results/1_5.png}
        \end{center}
        As seen in the previous question, the principal components upto $20$ express most of the variance in the dataset which is necessary to capture the features/characteristics of the dataset and differentiate between classes. It has been discussed in the lectures, that really dark and light/bright areas in the plot correspond to the dominating factors with which we differentiate classes. The gray and dull areas don't affect the differentiation a lot. In this plot we can see most of the subplots have clear images of some upper-body wear like shirts, t-shirts, coats, pullovers or footwear like sneakers, sneakers and boots. This is again something we would expect, as considering the classes of the dataset ($\frac{4}{10}$ upper-body wear, $\frac{3}{10}$ are footwear) these are the shapes/patterns which contain most of the variance. In some of the subplots we can vaguely see the outline of other classes like dresses as well.
    \end{answerbox}
  


   \end{subquestion}

   %==============================
   % Q1.6
   \begin{subquestion}{(5 points)
       Using \texttt{Xtrn\_nm}, 
       for each class and for each number of principal components $K =
       5, 20, 50, 200$, apply dimensionality reduction with PCA to the
       first sample in the class, reconstruct the sample from the
       dimensionality-reduced sample, and 
       report the Root Mean Square Error (RMSE) between the
       original sample in {\tt Xtrn\_nm} and reconstructed one.
     } \label{Q1.6}

     

      \begin{answerbox}{25em}
         \centering
        \begin{tabular}{|c|l|l|l|l|l|l|l|l|}
            \hline
            \textbf{Class} & \textbf{K} & \textbf{RSME} & \textbf{K} & \textbf{RSME} & \textbf{K} & \textbf{RSME} & \textbf{K} & \textbf{RSME}   \\ \hline
            0     & 5 & 0.2561 & 20 & 0.1501 & 50 & 0.127 & 200 & 0.0619 \\ \hline
            1     & 5 & 0.198 & 20 & 0.1404 & 50 & 0.0952 & 200 & 0.0361 \\ \hline
            2     & 5 & 0.1987 & 20 & 0.1455 & 50 & 0.1224 & 200 & 0.0798 \\ \hline
            3     & 5 & 0.1457 & 20 & 0.1075 & 50 & 0.0838 & 200 & 0.0561 \\ \hline
            4     & 5 & 0.1182 & 20 & 0.1026 & 50 & 0.087 & 200 & 0.0466 \\ \hline
            5     & 5 & 0.1811 & 20 & 0.1586 & 50 & 0.1431 & 200 & 0.0898 \\ \hline
            6     & 5 & 0.1295 & 20 & 0.0963 & 50 & 0.0725 & 200 & 0.0446 \\ \hline
            7     & 5 & 0.1656 & 20 & 0.1272 & 50 & 0.1059 & 200 & 0.0614 \\ \hline
            8     & 5 & 0.2234 & 20 & 0.1449 & 50 & 0.1233 & 200 & 0.093 \\ \hline
            9     & 5 & 0.1835 & 20 & 0.1513 & 50 & 0.1218 & 200 & 0.0716 \\ \hline
        \end{tabular}
      \end{answerbox}
  


   \end{subquestion}
   
   %==============================
   % Q1.7
   \begin{subquestion}{(4 points)
       Display the image for each of the reconstructed samples in
       a 10-by-4 grid, where each row corresponds to a class and
       each row column corresponds to a value of $K=5, \; 20, \; 50, \; 200$.
     } \label{Q1.7}


   

      \begin{answerbox}{52em}
         \begin{center}
             \includegraphics[width=0.75\textwidth, height=37em]{results/1_7.png}
         \end{center}
         We can see in the plot that more the number of principal components, more is the clarity and definition of the image. On the other hand we can also observe that even when using low values of $K$ like $5$ or $20$, we are still able to capture many of the characteristics. This is logical as we have seen in previous questions as how by increasing the number of components we can capture a lot more variance in the data but the first few components alone can explain most of the variance in the dataset. From the previous question we can also say that the RSME value decreases as the number of component increases and we can see how our images are more sharper and truer to their class.
      \end{answerbox}
  


   \end{subquestion}
   %==============================
   %
   %==============================
   % Q1.8
   \begin{subquestion}{(4 points)
       Plot all the training samples (\texttt{Xtrn\_nm}) on the
       two-dimensional PCA plane you obtained in \refQ{Q1.pca.variance}, where each sample is
       represented as a small point with a colour specific to the class of
       the sample.  Use the 'coolwarm' colormap for plotting.
     } \label{Q1.8}


   

      \begin{answerbox}{40em}
         \begin{center}
             \includegraphics[width=0.75\textwidth]{results/1_8.png}
         \end{center}
         The data is quite clearly not linearly separable, but from this scatter plot of the projection of training samples see can see that most samples of each class are clustered together. We can also clearly identify a few more characteristic of the plot; similar classes are clustered near each other, i.e. upper body wear is scattered downwards while footwear classes are projected upwards of the graph.
      \end{answerbox}
  


   \end{subquestion}
   %
   %==============================
   

\end{question}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%============================================================================%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
%
% Question 2
%
\begin{question}{(25 total points) Logistic regression and SVM}

  \questiontext{In this question we will explore 
    classification of image data with logistic regression and support
    vector machines (SVM) and visualisation 
    of decision regions.
  }
  


  \medskip
   %==============================
   % Q2.1
   \begin{subquestion}{(3 points)
       Carry out a classification experiment with
       \href{https://scikit-learn.org/0.19/modules/generated/sklearn.linear\_model.LogisticRegression.html}{multinomial logistic regression},
       and report the classification accuracy and confusion matrix (in
       numbers rather than in graphical representation such as heatmap)
       for the test set.
     } \label{Q2.1}


   

      \begin{answerbox}{30em}
        
        \textbf{Table of confusion matrix for the test set}
        \medskip
        \centering
        \small
        \begin{tabular}{@{}lllllllllll@{}}
        \toprule
             & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \midrule
            0 & 819 & 3 & 15 & 50 & 7 & 4 & 89 & 1 & 12 & 0 \\ 
            1 & 5 & 953 & 4 & 27 & 5 & 0 & 3 & 1 & 2 & 0 \\ 
            2 & 27 & 4 & 731 & 11 & 133 & 0 & 82 & 2 & 9 & 1 \\ 
            3 & 31 & 15 & 14 & 866 & 33 & 0 & 37 & 0 & 4 & 0 \\ 
            4 & 0 & 3 & 115 & 38 & 760 & 2 & 72 & 0 & 10 & 0 \\ 
            5 & 2 & 0 & 0 & 1 & 0 & 911 & 0 & 56 & 10 & 20 \\ 
            6 & 147 & 3 & 128 & 46 & 108 & 0 & 539 & 0 & 28 & 1 \\ 
            7 & 0 & 0 & 0 & 0 & 0 & 32 & 0 & 936 & 1 & 31 \\ 
            8 & 7 & 1 & 6 & 11 & 3 & 7 & 15 & 5 & 945 & 0 \\ 
            9 & 0 & 0 & 0 & 1 & 0 & 15 & 1 & 42 & 0 & 941 \\ \bottomrule 
        \end{tabular}

        \medskip
        \noindent
        \textbf{Classification Accuracy:}
        $$84.01\%$$
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
   %==============================
   % Q2.2
   \begin{subquestion}{(3 points)
       Carry out a classification experiment with
       \href{https://scikit-learn.org/0.19/modules/generated/sklearn.svm.SVC.html}{SVM classifiers}, and report the
       mean accuracy and confusion matrix (in numbers) for the test
       set.
     } \label{Q2.2}


   

      \begin{answerbox}{30em}
        \textbf{Table of confusion matrix for the test set}
        \bigskip
        \small
        \centering
        \begin{tabular}{@{}lllllllllll@{}}
        \toprule
             & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \midrule
            0 & 845 & 2 & 8 & 51 & 4 & 4 & 72 & 0 & 14 & 0 \\ 
            1 & 4 & 951 & 7 & 31 & 5 & 0 & 1 & 0 & 1 & 0 \\ 
            2 & 15 & 2 & 748 & 11 & 137 & 0 & 79 & 0 & 8 & 0 \\ 
            3 & 32 & 6 & 12 & 881 & 26 & 0 & 40 & 0 & 3 & 0 \\ 
            4 & 1 & 0 & 98 & 36 & 775 & 0 & 86 & 0 & 4 & 0 \\ 
            5 & 0 & 0 & 0 & 1 & 0 & 914 & 0 & 57 & 2 & 26 \\ 
            6 & 185 & 1 & 122 & 39 & 95 & 0 & 533 & 0 & 25 & 0 \\ 
            7 & 0 & 0 & 0 & 0 & 0 & 34 & 0 & 925 & 0 & 41 \\ 
            8 & 3 & 1 & 8 & 5 & 2 & 4 & 13 & 4 & 959 & 1 \\ 
            9 & 0 & 0 & 0 & 0 & 0 & 22 & 0 & 47 & 1 & 930 \\ \bottomrule 
        \end{tabular}
        
        \medskip
        \noindent
        \textbf{Classification Accuracy:}
        $$84.61\%$$

      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
   %==============================
   % Q2.3
   \begin{subquestion}{(6 points)
       We now want to visualise the decision regions for the logistic
       regression classifier we trained in \refQ{Q2.1}.
     } \label{Q2.3}


   

      \begin{answerbox}{35em}
         \begin{center}
             \includegraphics[width=0.9\textwidth]{results/2_3.png}
         \end{center}
         Logistic regression combines linear weights and a logistic squashing function. Being a linear classifier we expect the decision boundary to be a line and we can see that in the plot. We can see that how some classes are densely clustered together like we saw in the 2D-PCA plot. One major thing we can see is that there only 9 decision boundary regions in the plot. The missing decision region probably comes from the fact that a logistic classifier does not work very well with non-linear and unstructured/semi-structured data. It is not able to separate out the missing region from other classes.
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
   %==============================
   % Q2.4
   \begin{subquestion}{(4 points)
       Using the same method as the one above, plot the decision regions for
       the SVM classifier you trained in \refQ{Q2.2}.
       Comparing the result with that you obtained in \refQ{Q2.3}, discuss your
       findings briefly.
     } \label{Q2.4}
   

      \begin{answerbox}{35em}
         \begin{center}
               \includegraphics[width=0.9\textwidth]{results/2_4.png}
         \end{center}
         SVM classifiers try to find the "maximum" or the "best" margin between to classes to separate them. The "kernel trick" allows the SVM classifers to efficiently draw boundaries for non-linearly separable classes, and our decision boundaries are no longer only straight lines. Comparing this plot to our plot in \refQ{Q2.2} we can see that here decision regions for all classes are present and we have stronger correlation between decision boundaries and the 2D-PCA plot in \refQ{Q1.8}. 
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %

   %==============================
   % Q2.5
   \begin{subquestion}{(6 points)
       We used default parameters for the SVM in \refQ{Q2.2}.
       We now want to tune the parameters by using cross-validation.
       To reduce the time for experiments, you pick up the first 1000
       training samples from each class to create \texttt{Xsmall}, so that \texttt{Xsmall}
       contains 10,000 samples in total. Accordingly, you create
       labels, \texttt{Ysmall}.
     } \label{Q2.5}


   

      \begin{answerbox}{30em}
        \begin{center}
            \includegraphics[width=0.9\textwidth]{results/2_5.png}    
        \end{center}
        \textbf{Highest Obtained Mean Accuracy: } $85.65\%$
        \newline
        \textbf{Value of C: } $21.544$
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
   %==============================
   % Q2.6
   \begin{subquestion}{(3 points)
       Train the SVM classifier on the whole training set by using the
       optimal value of $C$ you found in \refQ{Q2.5}. 
     } \label{Q2.6}


       

      \begin{answerbox}{10em}
        \textbf{Accuracy of the improved SVM classifier:}
        \medskip
        \begin{center}
            \begin{tabular}{c|c}
            \toprule
              Train & Test \\ \hline
              $90.842\%$ & $87.65\%$ \\
            \bottomrule
         \end{tabular}
        \end{center}
        \textbf{Best C: }
            $$21.5443$$
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
%
%

\end{question}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%============================================================================%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
%
% Question 3
%

\begin{question}{(20 total points) Clustering and Gaussian Mixture Models}  


  \questiontext{In this question we will explore K-means clustering,
    hierarchical clustering, and GMMs.
  }
  


  \medskip
   %==============================
   % Q3.1
   \begin{subquestion}{(3 points)
       Apply k-means clustering on {\tt Xtrn} for $k = 22$, where we use
       \href{https://scikit-learn.org/0.19/modules/generated/sklearn.cluster.KMeans.html}{sklearn.cluster.KMeans}
       with the parameters {\tt n\_clusters=22} and {\tt random\_state=1}.
       Report the sum of squared distances of samples to their closest
       cluster centre, and the number of samples for each cluster.
     } \label{Q3.1}
   

      \begin{answerbox}{35em}
        Sum of squared distances of samples to their closest cluster centre = $38185.817$
         \begin{center}
            \begin{tabular}{|c|c|}
                \hline
                Cluster & Num of Samples \\ \hline
                0 & 1018 \\
                1 & 1125 \\
                2 & 1191 \\
                3 & 890 \\
                4 & 1162 \\
                5 & 1332 \\
                6 & 839 \\
                7 & 623 \\
                8 & 1400 \\
                9 & 838 \\
                10 & 659 \\
                11 & 1276 \\
                12 & 121 \\
                13 & 152 \\
                14 & 950 \\
                15 & 1971 \\
                16 & 1251 \\
                17 & 845 \\
                18 & 896 \\
                19 & 930 \\
                20 & 1065 \\
                21 & 1466 \\
                \hline
            \end{tabular}    
         \end{center}
         
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
   %==============================
   % Q3.2
   \begin{subquestion}{(3 points)
       Using the training set only,
       calculate the mean vector for each language, and plot the mean
       vectors of all the 22 languages on a 2D-PCA plane, where you
       apply PCA on the set of 22 mean vectors without applying
       standardisation.  
       On the same figure, plot the cluster centres obtained in \refQ{Q3.1}.
     } \label{Q3.2}

   

      \begin{answerbox}{35em}
         \begin{center}
             \includegraphics[width=0.85\textwidth]{results/3_2.png}
         \end{center}
         The cluster center is the mean of all the samples assigned to it. In the plot we can see many of the cluster centres are around their language means, except for the few outliers which are more spread out. In general, we can see that language means are more densely populated on the graph compared to the cluster centres. This could happen as KMeans algorithm picks k-random points to start the clustering and our clusters are affected by it. So yes we can say language clusters and language means are similar. 
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
   %==============================
   % Q3.3
   \begin{subquestion}{(3 points)
       We now apply hierarchical clustering on the training data set
       to see if there are any structures in the spoken languages.
     } \label{Q3.3}


     

      \begin{answerbox}{35em}
        \begin{center}
            \includegraphics[width=0.6\textwidth, height=21em]{results/3_3.png}
        \end{center}
        In this plot we a have dendrogram which visualizes the hierarchical clustering of data points using the Ward linkage method. Ward's method uses analysis of the variance in the data to calculate the distances between clusters/points. Lesser the distance the more similar we assume the data points to be. We can use these distances to find out the ideal number of cluster we will have. In this plot if consider the languages the number labels represent we can see a clear grouping of European latin derived languages together (14, 1, 10, 8, 5). This could be one of our clusters. However we some obvious errors as well, for eg. English and Mongolian (4, 13) being grouped together even though they have completely different roots.
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
   %==============================
   % Q3.4
   \begin{subquestion}{(5 points)
       We here extend the hierarchical clustering done in \refQ{Q3.3} by
       using multiple samples from each language.
     } \label{Q3.4}


   

      \begin{answerbox}{50em}
         \begin{center}
            \includegraphics[width=0.48\textwidth, height=16em]{results/3_4_complete.png}\hfill
            \includegraphics[width=0.48\textwidth, height=16em]{results/3_4_single.png}\hfill
            \includegraphics[width=0.5\textwidth, height=16em]{results/3_4_ward.png}
         \end{center}
         \begin{itemize}
             \item Single-linkage (nearest neighbor) is the shortest distance between a pair of observations in two clusters. Dendrogram of single-linkage tends to form long chains and can appear to be spread out. We can observe this in the red-coloured cluster of our plot.
             \item Complete-linkage (farthest neighbour) is where distance is measured between the farthest pair of observations in two clusters. A larger number of clusters are produced compared to single-linkage but these are much more closer to each other.
             \item Ward-linkage minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged. This leads to more dense clusters. 
         \end{itemize}
      \end{answerbox}
  


   \end{subquestion}
   %
   % ==============================
   %
   %==============================
   % Q3.5
   \begin{subquestion}{(6 points)
       We now consider Gaussian mixture model (GMM), whose
       probability distribution function (pdf) is given as
       a linear combination of Gaussian or normal distributions, i.e.,
     } \label{Q3.5}




      \begin{answerbox}{30em}
        \begin{center}
            \begin{tabular}{l|c|c|c|c|}
                \cline{2-5}
                 & \multicolumn{2}{c|}{\textbf{Diagonal Covaraince}} & \multicolumn{2}{c|}{\textbf{Full Covariance}} \\ \hline
                \multicolumn{1}{|l|}{\textbf{K}} & \textbf{Train}           & \textbf{Test}          & \textbf{Train}         & \textbf{Test}        \\ \hline
                \multicolumn{1}{|l|}{1}  & 14.280 & 13.843 & 16.394 & 15.811 \\ \hline
                \multicolumn{1}{|l|}{3}  & 15.398 & 15.041 & 18.096 & 17.066 \\ \hline
                \multicolumn{1}{|l|}{5}  & 16.010 & 15.909 & 19.036 & 16.489 \\ \hline
                \multicolumn{1}{|l|}{10} & 16.917 & 16.568 & 21.062 & 14.622 \\ \hline
                \multicolumn{1}{|l|}{15} & 17.505 & 16.902 & 22.786 & 11.848 \\ \hline
            \end{tabular}
        \end{center}
         \begin{center}
             \includegraphics[width=0.6\textwidth, height=10em]{results/3_5.png}
         \end{center}
         \begin{itemize}
             \item On the training data, the score is proportional to the number of mixtures in the GMM for both matrices.
             \item On test data, the score is proportional to the number of mixtures in the GMM for the diagonal covariance matrix and inversely proportional (after a few mixtures) for the full covariance matrix.
         \end{itemize}
    \end{answerbox}
  


   \end{subquestion}
   %
   %==============================

   % ==============================
   
\end{question}
\end{document}
